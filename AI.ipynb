{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0e8817-db0b-4cc0-9028-f61a5ae63167",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad7d83b-1de1-4103-a18e-8eba24cea43d",
   "metadata": {},
   "source": [
    "The machine learning process generally follows a structured, step-by-step approach, which can be broken down into three main phases:\n",
    "\n",
    "1. Data Pre-processing:\n",
    "    - Import the data\n",
    "    - Clean the data\n",
    "    - Encoding data\n",
    "    - Split the data into training and test sets\n",
    "    - Feature scaling\n",
    "2. Modeling:\n",
    "    - Build the machine learning model\n",
    "    - Train the model on the training data\n",
    "    - Make predictions using the model\n",
    "3. Evaluation:\n",
    "    - Calculate performance metrics to assess how well the model performs\n",
    "    - Determine if the model is a good fit for the data and fulfills its intended purpose\n",
    "\n",
    "This process is key to building effective machine learning models, and throughout the examples, you'll gain practical experience applying these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc681a-21fa-41d0-9ddf-b5bd66c14acb",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2d893-f29a-49c3-a137-aabbc144d341",
   "metadata": {},
   "source": [
    "We will explore in detail the **Pre-processing** process, which is common to all machine learning algorithms.\n",
    "It is important to master this pre-processing step before exploring the different algorithms.\n",
    "\n",
    "Data preprocessing is the crucial first step in any machine learning project because it transforms raw data into a clean, consistent format for the model:\n",
    "- Data Quality: Raw data often contains errors like missing values or duplicates. Preprocessing cleans this up to ensure the model learns from accurate data, leading to better predictions.\n",
    "- Consistency: Different formats across data sources can confuse the model. Preprocessing standardizes formats, ensuring consistent analysis.\n",
    "- Performance: Preprocessing highlights important features and reduces noise, enabling the model to find patterns more effectively and make more accurate predictions.\n",
    "- Efficiency: By simplifying data, preprocessing makes the model faster and more efficient, helping it focus on what matters most.\n",
    "\n",
    "Preprocessing ensures your data is clean, organized, and ready for the model, improving both accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35563657-4d8c-4871-9653-46ec5648d3ad",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83869adb-63a7-4589-95f2-c0ebf4096fe0",
   "metadata": {},
   "source": [
    "The first step in building machine learning models is to import the necessary libraries. In this case, we’ll work with three essential libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e9a3324-2271-4026-bf52-2f11b7a8d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy helps us work with arrays, which are commonly used as input for machine learning models.\n",
    "# It is typically imported with the shortcut np.\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib creates charts and graphs.\n",
    "# It is usually imported with the shortcut plt.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pandas is used for data manipulation and preprocessing, such as importing datasets\n",
    "# and creating matrices of features and dependent variables.\n",
    "# It is commonly imported with the shortcut pd.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea60adb-6c74-4858-964f-f31313431e16",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ece0a9-2426-4ff2-a74a-51576c7b6186",
   "metadata": {},
   "source": [
    "To begin data pre-processing in machine learning, the first step is to import a dataset. In this example, we are working with a CSV file, data.csv, which contains customer information for a retail company, including country, age, salary, and whether they purchased a product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c4ccbb6-9e5f-4ffe-af8c-3e7210cf56d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the read_csv function from Pandas to import the dataset, which is typically a CSV file\n",
    "# This reads the file and stores it as a data frame, which holds all rows and columns of data from the CSV.\n",
    "dataset = pd.read_csv('./pp/Data.csv')\n",
    "dataset.head() # display the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b17d4-b5c1-4fc6-b2e2-8303b41ac6b0",
   "metadata": {},
   "source": [
    "In any machine learning model, the dataset is split into two parts:\n",
    "- **Features** (Independent Variables): The information used to make predictions, typically the columns other than the last one.\n",
    "- **Dependent Variable**: The target outcome we want to predict, typically the last column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78009775-61cf-4215-a771-6f673a0b67cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the features \"X\" include country, age, and salary\n",
    "X = dataset.iloc[:, :-1].values # get all the rows and get all the column except the last one\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dee0ddc2-1435-449a-8bf8-eebd5fcd6252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dependent variable \"y\" is whether the customer purchased the product or not\n",
    "y = dataset.iloc[:, -1].values # get all the rows and get only the last column\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676a86c-786a-4d52-ad80-0df8de2fc47f",
   "metadata": {},
   "source": [
    "### Taking care of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666a842-250d-4e5c-9b09-5517c692733c",
   "metadata": {},
   "source": [
    "Missing data can cause errors when training machine learning models, so it’s important to handle it properly. There are two common approaches:\n",
    "\n",
    "1. Ignoring or Deleting Missing Data:\n",
    "This method works well when the amount of missing data is minimal (e.g., less than 1%). Deleting a small percentage of data may not significantly impact the model's performance.\n",
    "\n",
    "2. Replacing Missing Data:\n",
    "A more common and reliable approach is to replace missing values with a calculated statistic, such as the mean of the column. This ensures that no data is lost, especially when there are large amounts of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d069d4fb-f085-4266-a348-dd59a1566c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can see here, we have missing age for Spain and missing salary for Germany\n",
    "# We want to replace this missing salary and age by the average\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2e1b5d2-e068-46bd-be42-be95c4528ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the library that handle missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an instance of SimpleImputer specifying the replacement strategy.\n",
    "# In this case we replace all missing values \"missing_values=np.nan\" with the mean \"strategy='mean'\"\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# The fit() method connects the imputer to the data by analyzing the columns with\n",
    "# numerical values and calculating statistics like the mean or median.\n",
    "# We give our last two columns that contains age and salary to the imputer\n",
    "imputer.fit(X[:, 1:3])\n",
    "\n",
    "# The transform() method applies the transformation to the data by replacing missing values\n",
    "# with the calculated statistics. The result is an updated matrix of features with no missing values.\n",
    "# Then we update the original matrix X, replacing the missing values in the specified columns with the mean values.\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "\n",
    "# By using both the fit() and transform() methods, you ensure that your matrix of features\n",
    "# is complete and ready for further processing, avoiding errors\n",
    "# in your machine learning model caused by missing data.\n",
    "# As we can see here, all the missing values were replaced by the mean of each column.\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff3354-311e-408a-a9c0-0a387dfcac6a",
   "metadata": {},
   "source": [
    "### Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3600f-235d-44c9-a0a7-e199258a41b6",
   "metadata": {},
   "source": [
    "In machine learning, we often need to convert categorical data (like country names) into numerical data so that models can process them. However, a simple numerical encoding (e.g., France = 0, Spain = 1, Germany = 2) can create unintended relationships between categories that don’t exist, as the model may incorrectly assume an order between these numbers.\n",
    "\n",
    "To avoid this issue, we use **one-hot encoding**, which transforms a single categorical column into multiple binary columns. For example, the \"country\" column with values like \"France,\" \"Spain,\" and \"Germany\" is converted into three binary columns:\n",
    "\n",
    "- France: **[1, 0, 0]**\n",
    "- Spain: **[0, 1, 0]**\n",
    "- Germany: **[0, 0, 1]**\n",
    "\n",
    "This method removes any implied numerical order between categories.\n",
    "\n",
    "Additionally, binary categorical data (such as \"yes\" or \"no\" values in the \"purchased\" column) can be safely encoded as 0 and 1 without negatively impacting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37c9196b-8f65-4a44-bcf5-3a22ea0f19f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To perform one-hot encoding in Python, we use the ColumnTransformer and OneHotEncoder\n",
    "# These tools automate the conversion of categorical columns into binary columns,\n",
    "# allowing machine learning models to interpret and process them correctly.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Step 1: Let's now encode the features \"X\"\n",
    "\n",
    "\"\"\"\n",
    "We start by creating an object of the ColumnTransformer class.\n",
    "This object is responsible for applying transformations to specific columns.\n",
    "\n",
    "In the transformers argument, we specify three things:\n",
    "  - The type of transformation (e.g., \"encoder\" for one-hot encoding).\n",
    "  - The transformer to apply (e.g., OneHotEncoder()).\n",
    "  - The index of the column to transform (e.g., the \"country\" column, which has index 0).\n",
    "\n",
    "We also set the remainder argument to \"passthrough\" to ensure that columns not being transformed (e.g., age and salary) are retained.\n",
    "\"\"\"\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "\n",
    "\n",
    "# the fit_transform() method is used\n",
    "# to fit the ColumnTransformer to the data and apply the transformation in one step.\n",
    "# The result is an updated matrix of features with one-hot encoded columns.\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "# Since machine learning models expect the feature matrix (X) as a NumPy array,\n",
    "# we convert the result of fit_transform() to a NumPy array using np.array().\n",
    "X = np.array(X)\n",
    "\n",
    "# This process efficiently transforms categorical data into a suitable format\n",
    "# for machine learning models while ensuring that the rest of the feature matrix remains intact.\n",
    "# Let's check how our countries were transformed\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2bfb01d2-98f0-48f8-ac96-026b70073483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Let's now encode the dependant variable \"y\"\n",
    "\n",
    "\"\"\"\n",
    "for binary categorical data in the dependent variable (e.g., \"yes\" and \"no\"),\n",
    "we use label encoding.\n",
    "Label encoding converts these text values into binary numerical values:\n",
    "\n",
    "  - \"No\"  becomes 0\n",
    "  - \"Yes\" becomes 1\n",
    "\"\"\"\n",
    "\n",
    "# This transformation is done using the LabelEncoder class from scikit-learn.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# The fit_transform() method quickly converts the text into corresponding\n",
    "# numerical values for the dependent variable vector, which is now ready for model training.\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# By label encoding for binary outcomes, you ensure that categorical data is\n",
    "# in a format that machine learning models can process effectively.\n",
    "# Let's check the result of the transformation\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80771e9f-a757-44bb-82b8-2a0f26a14505",
   "metadata": {},
   "source": [
    "### Splitting the dataset into Training set and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4d8e7-2be5-4620-b903-80460feb3393",
   "metadata": {},
   "source": [
    "When preparing data for machine learning models, there are two key steps: **splitting the dataset** into training and test sets, and applying **feature scaling**.\n",
    "\n",
    "1. Splitting the Dataset, the dataset is split into two parts:\n",
    "    - Training Set: Used to train the model on existing observations.\n",
    "    - Test Set: Used to evaluate the performance of the model on new, unseen data, simulating future predictions.\n",
    "2. Feature Scaling:\n",
    "    - Feature scaling ensures that all features are on the same scale to prevent one feature from dominating others. This step adjusts the variables so that they take values within the same range, usually by normalizing or standardizing the data.\n",
    "\n",
    "**Feature Scaling must be applied AFTER splitting the dataset**\n",
    "\n",
    "The reason for this is to avoid **information leakage**. The test set is meant to represent new, unseen data. If you apply feature scaling before the split, you would be calculating the mean and standard deviation using the entire dataset, including the test set. This would allow the model to \"see\" the test data during training, which defeats the purpose of having a separate test set and leads to an inaccurate evaluation of model performance.\n",
    "\n",
    "By applying feature scaling after the split, you ensure that the test set remains untouched during training, preserving its role as new data to validate the model’s true performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9a985f4b-cc19-488b-a53d-3255e622363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To split the dataset, we use the train_test_split function from scikit-learn’s model_selection module.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "The function train_test_split splits the dataset into four sets:\n",
    "  X_train: Matrix of features for the training set.\n",
    "  X_test:  Matrix of features for the test set.\n",
    "  y_train: Dependent variable for the training set.\n",
    "  y_test:  Dependent variable for the test set.\n",
    "\n",
    "The main parameters are:\n",
    "  X: Matrix of features.\n",
    "  y: Dependent variable vector.\n",
    "  test_size: Specifies the proportion of data to allocate to the test set (e.g., 20% or 0.2).\n",
    "  random_state: A fixed seed to ensure reproducibility of the split.\n",
    "\n",
    "The function returns four sets: X_train, X_test, y_train, and y_test.\n",
    "Typically, 80% of the data is assigned to the training set and 20% to the test set.\n",
    "This ratio ensures the model has enough data to learn from while still reserving a portion for evaluation.\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dcb7070c-2af8-452b-a95e-31feaab3e9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0]], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3b2ca6b6-c5b5-41f7-a680-7864608062c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0]], dtype=object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ee1255aa-c10d-469c-ba58-f1612d1febd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7e0c51f8-534b-4557-a7ed-e7c9a45c9bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e804785-18fe-4413-b39b-cdd2b66bb09a",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490f1f0-6c2c-4e72-be03-6cc34c13808a",
   "metadata": {},
   "source": [
    "**Feature scaling** is the process of putting all features on the same scale to prevent certain features from dominating others due to differences in their magnitudes. However, feature scaling is not necessary for all machine learning models but is crucial for models sensitive to the scale of data, such as gradient-based algorithms (e.g., logistic regression, support vector machines).\n",
    "\n",
    "There are two Main Techniques:\n",
    "- Standardization:\n",
    "  - Formula: Subtract the mean of the feature and divide by the standard deviation.\n",
    "  - Result: Values are typically scaled between -3 and +3.\n",
    "  - Works well for all types of data, making it a reliable go-to method.\n",
    "- Normalization:\n",
    "  - Formula: Subtract the minimum value of the feature and divide by the range (maximum - minimum).\n",
    "  - Result: Values are scaled between 0 and 1.\n",
    "  - Best used when features follow a normal distribution.\n",
    "\n",
    "Feature scaling should always be applied **after splitting** the dataset into training and test sets. This is to prevent information leakage from the test set into the training process. Feature scaling is fitted on the training data (X_train), and then the same scaling transformation is applied to the test data (X_test), using the mean and standard deviation calculated from X_train.\n",
    "\n",
    "Practical Recommendation:\n",
    "    - Standardization is generally preferred because it works well across various scenarios and always improves model performance by ensuring consistent feature scaling.\n",
    "    - Normalization is ideal when you have normally distributed data.\n",
    "\n",
    "Standardization should only be applied to numerical features, not to dummy (one-hot encoded) variables.\n",
    "Dummy variables (e.g., binary 0s and 1s representing categorical data)\n",
    "are already on the same scale, and applying scaling to them would distort the interpretation of these values.\n",
    "\n",
    "Dummy variables already take values between 0 and 1, so scaling them is unnecessary\n",
    "and could result in the loss of interpretability.\n",
    "For instance, you may lose the ability to identify which binary values represent specific categories\n",
    "(e.g., countries). Moreover, applying scaling to dummy variables\n",
    "does not significantly improve model performance, and in most cases,\n",
    "it can even make the interpretation harder.\n",
    "\n",
    "Thus, feature scaling should be applied only to the numerical variables\n",
    "(e.g., age, salary) where the scale differs significantly.\n",
    "This ensures that your machine learning models are properly trained\n",
    "without distorting categorical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "df0b032b-3b77-4f09-87d6-5924e1435d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply feature scaling, we use the StandardScaler class from the scikit-learn library,\n",
    "# which standardizes the features by subtracting the mean and dividing by the standard deviation.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initiate the scaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "\"\"\"\n",
    "X_train[:, 3:] means that we take all the rows and only the age and salary columns\n",
    "\n",
    "The first step is to fit the scaler on the training set (X_train) to compute\n",
    "the mean and standard deviation for each numerical feature.\n",
    "This process ensures that the scaling is based solely on the training data.\n",
    "You only need to scale the numerical columns.\n",
    "\n",
    "In this case, the numerical columns are the age and salary columns,\n",
    "which have indexes 3 and 4, respectively.\n",
    "We exclude the dummy (one hot encoded) variables from scaling.\n",
    "\"\"\"\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "\n",
    "\"\"\"\n",
    "After applying feature scaling to the training set (X_train),\n",
    "the next step is to apply the same transformation to the test set (X_test).\n",
    "This is crucial to ensure that the test data is scaled using the same parameters\n",
    "(mean and standard deviation) computed from the training set.\n",
    "\n",
    "Since the test set simulates new, unseen data, you should only apply the transform()\n",
    "method (not fit_transform()), using the scaler that was fitted on the training set.\n",
    "This ensures consistency in scaling between the training and test sets,\n",
    "and prevents the model from learning any information from the test data (avoiding information leakage).\n",
    "\"\"\"\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195e5a6-6d65-40f2-b422-5e9840ee5347",
   "metadata": {},
   "source": [
    "After applying the scaling to both X_train and X_test, you can print them to verify that the numerical columns (e.g., age and salary) are now scaled between a common range, typically between -3 and +3 or -2 and +2, while dummy variables remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "75d38775-b078-43f6-8b8e-d40afd2702ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, -0.7529426005471072, -0.6260377781240918],\n",
       "       [1.0, 0.0, 0.0, 1.008453807952985, 1.0130429500553495],\n",
       "       [1.0, 0.0, 0.0, 1.7912966561752484, 1.8325833141450703],\n",
       "       [0.0, 1.0, 0.0, -1.7314961608249362, -1.0943465576039322],\n",
       "       [1.0, 0.0, 0.0, -0.3615211764359756, 0.42765697570554906],\n",
       "       [0.0, 1.0, 0.0, 0.22561095973072184, 0.05040823668012247],\n",
       "       [0.0, 0.0, 1.0, -0.16581046438040975, -0.27480619351421154],\n",
       "       [0.0, 0.0, 1.0, -0.013591021670525094, -1.3285009473438525]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d64f7bec-27ca-427f-8c7d-300434c47ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 2.1827180802863797, 2.3008920936249107],\n",
       "       [0.0, 0.0, 1.0, -2.3186282969916334, -1.7968097268236927]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d63bb-6962-4ff3-bbbb-ae25c1d11fe2",
   "metadata": {},
   "source": [
    "By properly scaling both the training and test sets, you ensure that your machine learning models are trained and evaluated under consistent conditions, leading to more accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa7f04-5abb-4433-b8ff-b19591eec576",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1fbaf-0ce0-4a17-9099-3a3f2668220c",
   "metadata": {},
   "source": [
    "The Internet is a massive global network that has transformed how we communicate and access information. In the early 90s, the concept of the Internet was new, and many people didn’t fully understand its potential. Back then, it was seen as something abstract and futuristic, but today it is a crucial part of our everyday lives. It connects individuals, organizations, and resources worldwide, allowing communication, sharing, and collaboration on an unprecedented scale.\n",
    "\n",
    "Neural networks and deep learning, much like the Internet, have existed for quite some time. The foundations of these technologies were laid in the 1960s and 70s, and by the 1980s, they were generating significant interest. Despite early enthusiasm, neural networks failed to deliver on their promise at that time, largely due to technological limitations. There wasn’t enough data, and computers were not powerful enough to process information at the scale required for neural networks to perform well. However, recent advancements in technology have led to a resurgence of interest in these fields, especially with the rise of deep learning, which is now transforming industries worldwide.\n",
    "\n",
    "The evolution of data storage has played a major role in this resurgence. In 1956, a 5 MB hard drive was so large that it had to be transported via forklift and way too expensive. Fast forward to nowadays, you could buy a 1 TB SSD that fits on your fingertip for a few bucks. This rapid expansion in storage capacity is part of the exponential growth that has defined technological progress in recent decades.\n",
    "\n",
    "Alongside data storage, the growth in computing power has followed Moore's Law, which observes that the processing power of computers doubles roughly every two years. In the 2020s, computers have achieved processing speeds comparable to the brain of a small mammal, like a rat. This explosion in computing capability is one of the key factors behind the rise of deep learning today, making it possible to train models on massive datasets.\n",
    "\n",
    "Deep learning itself is inspired by the structure of the human brain. It relies on artificial neural networks, which consist of several layers. The first is the input layer, where data is fed into the model. This data is processed through several hidden layers, which mimic the brain’s network of neurons. Information in the human brain doesn’t travel directly from input (senses) to output (actions or decisions). Instead, it passes through billions of interconnected neurons. Similarly, in deep learning, input data passes through hidden layers before reaching the output layer, where predictions or decisions are made.\n",
    "\n",
    "While early versions of neural networks only had one hidden layer, deep learning uses many hidden layers, allowing it to process complex data in ways that resemble how the brain works. This multi-layered architecture is what makes deep learning so powerful today, enabling machines to handle tasks like image recognition, speech processing, and language translation with unprecedented accuracy.\n",
    "\n",
    "![image](./dl/img/neural_net_intro.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce9bde-12ab-48e4-8cbe-9ebcf1c0bc0a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf3751-9cfe-47a7-90fb-52a5a9843be0",
   "metadata": {},
   "source": [
    "### The neuron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c66ef1d-ddc4-4166-a3ed-e41c52bdf5e3",
   "metadata": {},
   "source": [
    "The human brain is an incredible learning mechanism, and by replicating it in artificial systems, we hope to create similarly powerful learning tools for machines.\n",
    "\n",
    "Our first step in building neural networks is to recreate a neuron in a computer.\n",
    "In an artificial neural network, a neuron is connected to other neurons through synapses. Dendrites act as receivers, while axons transmit signals. Although axons and dendrites do not physically touch, they communicate across synapses, allowing the transfer of signals. In artificial networks, we simplify this by using input and output signals, representing synapses. We do not distinguish between axons and dendrites in the same way as we do in biology. Instead, we focus on the flow of information through the network, which is conceptually similar to how the brain operates.\n",
    "\n",
    "Neurons in machines receive input signals from other neurons, process these signals, and transmit an output signal. The input layer represents the values that are fed into the network, similar to how our senses provide information to the brain. In deep learning, the input values could be variables such as age, income, or any data relevant to the task at hand. These values are passed through the network, and each neuron's output contributes to the final result.\n",
    "\n",
    "It is important to standardize or normalize input values so that they fall within a similar range. This makes it easier for the neural network to process the data. There are different methods to achieve this, such as standardizing to a mean of zero and a variance of one, or normalizing data to fit within a range of zero to one. Both methods are essential for ensuring that the neural network functions optimally.\n",
    "\n",
    "The output of a neuron can be continuous (such as predicting a price), binary (such as predicting if a customer will leave a bank), or categorical (for multi-class classification tasks). In the case of categorical variables, the network generates multiple outputs representing each category.\n",
    "\n",
    "Each neuron is connected to other neurons through synapses, and these connections are assigned weights. The weights are essential because they determine the strength of the signal passed between neurons. The process of training a neural network involves adjusting these weights, allowing the network to learn which inputs are important and which are not. This is achieved through processes like backpropagation and gradient descent.\n",
    "\n",
    "Inside the neuron, the first step is to compute a weighted sum of all the input signals it receives. This sum is then processed through an activation function, which determines whether the neuron will pass the signal forward. This activation function plays a critical role in the network's ability to learn and make decisions. As signals pass through multiple neurons in the network, they undergo this process repeatedly, allowing the network to learn from data and make predictions.\n",
    "\n",
    "In summary, the neuron in an artificial neural network works by receiving input, applying weights, computing a sum, and passing the signal forward based on the activation function. This process is repeated across many neurons and synapses, enabling the network to learn from data and make accurate predictions.\n",
    "\n",
    "![image](./dl/img/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05623d9c-5057-449f-8cba-8279d917e30a",
   "metadata": {},
   "source": [
    "### The activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ed164-eefb-484b-bc0d-87fdce898c3c",
   "metadata": {},
   "source": [
    "We're going to cover activation functions which play a crucial role in how signals are passed from one neuron to another in neural networks. \n",
    "\n",
    "There are several activation functions available, but we will focus on four key types commonly used in neural networks.\n",
    "\n",
    "The first is the **threshold function**, which is simple and rigid. If the weighted sum is less than zero, the output is zero; if it’s equal to or greater than zero, the output is one. This function acts as a binary switch but can be limiting due to its lack of flexibility.\n",
    "\n",
    "![image](./dl/img/treshold_function.png)\n",
    "\n",
    "Next is the **sigmoid function**, which follows a smooth curve. Its formula is 1/(1 + e^(-x)), where x represents the weighted sum. The sigmoid function is especially useful in output layers when predicting probabilities. Unlike the threshold function, the sigmoid provides a gradual transition between outputs, making it more flexible for binary classification problems.\n",
    "\n",
    "![image](./dl/img/sigmoid_function.png)\n",
    "\n",
    "The third is the **rectifier function**, or **ReLU** (Rectified Linear Unit), which has become one of the most popular activation functions in artificial neural networks. It outputs zero for negative values and gradually increases for positive input values. Despite its simple structure, ReLU is highly effective and widely used in hidden layers of neural networks due to its efficiency in handling large datasets.\n",
    "\n",
    "![image](./dl/img/rectifier_function.png)\n",
    "\n",
    "Finally, we have the **hyperbolic tangent (tanh) function**, which is similar to the sigmoid function but with outputs ranging from -1 to 1. This range can be useful in certain applications where negative values are required, offering more flexibility than the sigmoid function.\n",
    "\n",
    "![image](./dl/img/hyperbolic_function.png)\n",
    "\n",
    "These four activation functions serve different purposes depending on the network's architecture and the type of problem you're trying to solve. For binary classification tasks, the threshold and sigmoid functions are typically used. For example, the threshold function fits perfectly for binary variables, as it can only output zero or one. The sigmoid function, on the other hand, can also be applied in such cases but works by providing the probability of the output being one, similar to logistic regression.\n",
    "\n",
    "In more complex networks with multiple layers, it’s common to use a combination of activation functions. Typically, in hidden layers, the rectifier function is applied to process intermediate signals, while in the output layer, a sigmoid function is used to predict probabilities. This combination is effective and frequently used in neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca64bf6-be8a-4d0c-a5f0-e46e6aa3eda5",
   "metadata": {},
   "source": [
    "### Neural Networks Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3681cba-2fb4-4b0b-a8fb-bcad7dc4407f",
   "metadata": {},
   "source": [
    "With neural networks, you create a structure that allows the program to learn by itself. Instead of defining rules, you provide inputs and outputs, and the network figures out how to achieve the desired result on its own. A good analogy for this is teaching a network to distinguish between cats and dogs. In a rule-based approach, you'd define specific features like pointy ears for a cat or droopy ears for a dog. In contrast, a neural network learns by analyzing pre-labeled images and figures out the distinguishing features on its own.\n",
    "\n",
    "Let’s now look at a simple neural network called a perceptron. A perceptron was invented in 1957 by Frank Rosenblatt and is the most basic neural network structure. It consists of input values, weighted sums, and an output value, referred to as ŷ (y hat).\n",
    "\n",
    "![image](./dl/img/perceptron.png)\n",
    "\n",
    "To help the perceptron learn, we calculate the difference between the predicted output and the actual output using a **cost function**, which measures the error in the network’s predictions. A common cost function is half the squared difference between the predicted and actual values. The goal is to **minimize this cost function**, as a lower value means the network’s predictions are more accurate.\n",
    "\n",
    "![image](./dl/img/perceptron_1.png)\n",
    "\n",
    "After calculating the error, we use that information to update the weights in the network. Adjusting the weights allows the network to improve its predictions on the next iteration. Initially, we use one row of data to train the network, tweaking the weights continuously until the predicted value is close to the actual value, and the cost function is minimized.\n",
    "\n",
    "![image](./dl/img/perceptron_2.png)\n",
    "\n",
    "Next, we move to a full dataset, which consists of multiple rows of inputs and actual outputs. Each row represents a new observation (e.g., student exam data), and we feed these rows one by one into the same neural network. After processing all rows, we calculate the overall cost function, which sums the squared differences between the predicted and actual values across all rows. The network then updates its weights based on the total error. This process of feeding the entire dataset into the network and updating the weights is called one epoch. The network repeats this process for many epochs until the cost function is minimized and the network learns the optimal weights.\n",
    "\n",
    "![image](./dl/img/perceptron_3.png)\n",
    "\n",
    "This method, known as **backpropagation**, is fundamental to training neural networks. It enables the network to learn from its mistakes by adjusting the weights in response to errors, gradually improving its predictions. The goal is to find the minimum cost function, which signifies that the network has found the best possible weights for the given dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f609591-64e6-4d79-ac4d-4a7338deabbf",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64476c-7e93-4c1c-8310-2cd7acaca7b6",
   "metadata": {},
   "source": [
    "We will explore **gradient descent** intuitively here. This is the key technique that neural networks use to adjust their weights and learn from data. Previously, we discussed backpropagation, where the error (the difference between predicted value ŷ is propagated back through the network, and the weights are adjusted to minimize this error. We will see exactly how these weight adjustments are made.\n",
    "\n",
    "To **minimize the cost function** (the measure of error), we use gradient descent. This method allows us to find the optimal weights by starting at a random point and using the slope of the cost function (its gradient) to guide us in the right direction. \n",
    "\n",
    "If the slope is negative, we move to the right (downhill):\n",
    "\n",
    "![image](./dl/img/gradient_descent_1.png)\n",
    "\n",
    "If the slope is positive, we move to the left (also downhill):\n",
    "\n",
    "![image](./dl/img/gradient_descent_2.png)\n",
    "\n",
    "This process continues iteratively, adjusting the weights step-by-step, until the minimum point of the cost function is reached.\n",
    "\n",
    "Think of gradient descent as rolling a ball down a hill. At each step, the ball moves in the direction of the steepest descent until it settles at the lowest point. In practice, gradient descent doesn’t follow a smooth path but instead takes a zigzag approach as it moves toward the minimum.\n",
    "\n",
    "![image](./dl/img/gradient_descent_3.png)\n",
    "\n",
    "Gradient descent can be applied in higher dimensions as well. In two and three-dimensional spaces, the method still works by moving through the cost function’s surface until it reaches the lowest point, or minimum.\n",
    "\n",
    "2D:\n",
    "\n",
    "![image](./dl/img/gradient_descent_4.png)\n",
    "\n",
    "3D:\n",
    "\n",
    "![image](./dl/img/gradient_descent_5.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b105e-7ce6-40de-97f0-831bc90f8e1e",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3ac6fb-3ff1-4947-87ef-a30b9168763b",
   "metadata": {},
   "source": [
    "We will be discussing **stochastic gradient descent** (SGD), a variation of gradient descent that is useful when the cost function is not convex. Previously, we learned that gradient descent is an efficient method for optimizing neural networks by minimizing the cost function. It allows us to determine the direction of steepest descent and iteratively adjust the weights to reach the minimum. However, gradient descent works best when the cost function is convex, meaning it has a single global minimum. If the cost function is non-convex, the algorithm may get stuck in a local minimum, which results in a suboptimal neural network.\n",
    "\n",
    "![image](./dl/img/stochastic_gradient_descent_1.png)\n",
    "\n",
    "This is where stochastic gradient descent comes into play. Unlike gradient descent, which adjusts weights after processing the entire dataset (batch gradient descent), SGD updates the weights after each row of data is processed. This approach introduces randomness into the learning process, helping the model avoid getting stuck in local minima and increasing the likelihood of finding the global minimum. Additionally, because SGD processes one row at a time, it is faster and requires less memory, making it a lighter and more efficient algorithm for large datasets.\n",
    "\n",
    "![image](./dl/img/stochastic_gradient_descent_2.png)\n",
    "\n",
    "The key difference between gradient descent and stochastic gradient descent lies in how frequently the weights are updated. In batch gradient descent, the weights are updated after the entire dataset is processed, while in stochastic gradient descent, they are updated after each individual row. This higher frequency of updates in SGD leads to greater fluctuations in the optimization path, allowing the model to escape local minima and better explore the cost function landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aae734-b206-47e5-a590-1b8e7a5d3f85",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa219728-89d3-4f59-8bc1-0810455cc824",
   "metadata": {},
   "source": [
    "We’ll discuss **backpropagation** in this part. It's the core algorithm used for training neural networks. As we’ve already learned, neural networks use a process called forward propagation to take input data, process it through layers, and produce an output value ŷ. This predicted value is then compared to the actual value from the training set, and the difference between them (the error) is calculated.\n",
    "\n",
    "**Backpropagation** is the process where this error is propagated backward through the network to adjust the weights. The unique advantage of backpropagation is that it allows the network to adjust all of the weights simultaneously, rather than updating them individually. This is a critical breakthrough that made neural networks viable for complex tasks.\n",
    "\n",
    "Now, let’s go over the steps involved in training a neural network using backpropagation:\n",
    "1. **Random Initialization**: The weights in the network are initialized to small, random values close to zero, but not zero. These weights will be adjusted as the network learns.\n",
    "2. **Input Data**: The first observation (or row) from the dataset is input into the neural network. Each feature is fed into a corresponding input node.\n",
    "3. **Forward Propagation**: The input values are propagated through the network, with each neuron’s activation influenced by the weights. This process continues until the network produces a predicted output ŷ.\n",
    "4. **Error Calculation**: The predicted output ŷ is compared to the actual output, and the error is calculated.\n",
    "5. **Backpropagation**: The error is propagated backward through the network, and the weights are updated based on their contribution to the error. The learning rate determines how much the weights are adjusted in each step.\n",
    "6. **Repeat**: The process of forward propagation, error calculation, and backpropagation is repeated for each observation. This can be done for each individual row (stochastic gradient descent), or after processing a batch of rows (batch or gradient descent).\n",
    "7. **Epochs**: Once the entire dataset has been processed, it is called an epoch. The network goes through multiple epochs, continuously adjusting the weights to minimize the cost function.\n",
    "\n",
    "These steps outline the basic procedure for training a neural network. Backpropagation allows the network to improve its performance by systematically adjusting its weights, leading to better and more accurate predictions as training progresses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b34a0d-117a-46d3-aaa1-334d60b4a7c9",
   "metadata": {},
   "source": [
    "## Artificial Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e368b57a-201b-4f19-9157-c82a2653f29a",
   "metadata": {},
   "source": [
    "We’re going to use TensorFlow 2.0 to build a deep neural network, complete with neurons and fully connected layers, and apply it to a real-world business problem.\n",
    "\n",
    "Our dataset comes from a bank, where we will work with various features about customers, such as their credit score, geography, age, balance, and more. The goal is to build a predictive model to classify whether a customer will stay with or leave the bank. The bank observed their customers over six months and collected information to understand why some left and others stayed. Once trained, our model will help the bank predict which customers are likely to leave, allowing them to take action to retain those customers.\n",
    "\n",
    "Import the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "ce8101d7-f0f9-474e-9986-b6761ca91bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.17.0'"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# > /dev/null     => it means that we don't want the output of the pip install command\n",
    "!pip install tensorflow > /dev/null \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "tf.__version__ # check tensorflow version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d11c1-e50f-41a0-8643-7053974489e3",
   "metadata": {},
   "source": [
    "The dataset contains some irrelevant columns, such as the row number, customer ID, and surname, which we exclude. We keep only the meaningful features like credit score, geography, age, balance, and so on. We then split the dataset into a matrix of features (X) and a dependent variable vector (y), where 'X' includes the features, and 'y' is the column indicating whether the customer stayed (0) or left (1).\n",
    "\n",
    "Import the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "f519789f-9018-4acf-85bf-429f030a842e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('./dl/Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:-1].values # [CreditScore, Geography, Gender, Age, Tenure, Balance, NumOfProducts, HasCrCard, IsActiveMember, EstimatedSalary]\n",
    "y = dataset.iloc[:, -1].values # [Exited]\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "03907217-91bc-4791-91c3-ba406a7bffe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', ..., 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', ..., 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [709, 'France', 'Female', ..., 0, 1, 42085.58],\n",
       "       [772, 'Germany', 'Male', ..., 1, 0, 92888.52],\n",
       "       [792, 'France', 'Female', ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "f83076d6-4322-4ca3-ada2-05a90fdc293b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7fdc3f-7bc2-4a6b-b2f8-2261ed16fc2b",
   "metadata": {},
   "source": [
    "Once the features are selected, we move on to encoding the categorical data. We first apply label encoding to the \"Gender\" column, converting it to numerical values (e.g., 0 for female, 1 for male). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "93629341-f86e-4f84-b51d-e5492416aa2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 0, ..., 1, 1, 101348.88],\n",
       "       [608, 'Spain', 0, ..., 0, 1, 112542.58],\n",
       "       [502, 'France', 0, ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [709, 'France', 0, ..., 0, 1, 42085.58],\n",
       "       [772, 'Germany', 1, ..., 1, 0, 92888.52],\n",
       "       [792, 'France', 0, ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "# Encode the gender clumn\n",
    "X[:, 2] = le.fit_transform(X[:, 2])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac8daf4-d455-4c14-abf7-57bef4aa1563",
   "metadata": {},
   "source": [
    "Then, we use one-hot encoding for the \"Geography\" column to handle the countries (France, Spain, Germany), ensuring the model treats them as distinct categories without any implied ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "cd7891db-9c06-4f70-8b35-1292d3b4ec28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, ..., 1, 1, 101348.88],\n",
       "       [0.0, 0.0, 1.0, ..., 0, 1, 112542.58],\n",
       "       [1.0, 0.0, 0.0, ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [1.0, 0.0, 0.0, ..., 0, 1, 42085.58],\n",
       "       [0.0, 1.0, 0.0, ..., 1, 0, 92888.52],\n",
       "       [1.0, 0.0, 0.0, ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encode the geography column\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8528563-10a8-4dea-b4e9-a1b762c9053b",
   "metadata": {},
   "source": [
    "We proceed to split the data into training and test sets, using 80% of the data for training and 20% for testing, ensuring that the model is trained on one set and evaluated on another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "53828082-3226-494b-a813-3ce0dcb96446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1aa75-8d81-41ac-8f37-934b0bc3008a",
   "metadata": {},
   "source": [
    "Finally, we apply feature scaling to normalize the values in all columns, which is essential for deep learning. By scaling all features, we ensure that the neural network performs optimally, as it requires values to be on a similar scale to work effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "1d694c5b-acb1-4a9e-8058-cf45c6851246",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "# feature scaling\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1edf34c-9286-46d4-876b-b774657c159d",
   "metadata": {},
   "source": [
    "We are now going to build the ANN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "344eb93d-b3cf-4d0a-b744-80deacfd2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start by initializing the ANN as a sequence of layers \n",
    "# using TensorFlow’s Sequential class, which organizes the layers in order.\n",
    "ann = tf.keras.models.Sequential()\n",
    "\n",
    "# Let's add all the layers !\n",
    "\n",
    "# After that, we add the input layer and the first hidden layer, \n",
    "# where we specify the number of neurons and choose an activation function, typically the ReLU\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
    "\n",
    "# To construct a deeper network,\n",
    "# we can easily add additional hidden layers by repeating this process.\n",
    "ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
    "\n",
    "# Once the hidden layers are set, we move on to the output layer. \n",
    "# For binary classification, the output layer will have a single neuron, \n",
    "# and we use the sigmoid activation function to produce \n",
    "# not only predictions but also probabilities for each class. \n",
    "ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc6ddf-ab45-42e9-8fd6-7998d8243141",
   "metadata": {},
   "source": [
    "Finally, after building the network structure, we’ll train the ANN by compiling it with an optimizer, loss function, and running the training over several epochs. This step turns the untrained network into a model capable of making intelligent predictions based on the data it’s trained on. With these steps, you’ve now successfully created your first artificial neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "99e3b753-7a30-491e-bf83-e4633b55ba01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323us/step - accuracy: 0.6118 - loss: 0.6614\n",
      "Epoch 2/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - accuracy: 0.7989 - loss: 0.4939\n",
      "Epoch 3/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - accuracy: 0.7992 - loss: 0.4597\n",
      "Epoch 4/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 448us/step - accuracy: 0.8017 - loss: 0.4372\n",
      "Epoch 5/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - accuracy: 0.8032 - loss: 0.4323\n",
      "Epoch 6/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - accuracy: 0.8182 - loss: 0.4197\n",
      "Epoch 7/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 436us/step - accuracy: 0.8075 - loss: 0.4282\n",
      "Epoch 8/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292us/step - accuracy: 0.8237 - loss: 0.4071\n",
      "Epoch 9/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - accuracy: 0.8272 - loss: 0.4001\n",
      "Epoch 10/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289us/step - accuracy: 0.8318 - loss: 0.3885\n",
      "Epoch 11/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - accuracy: 0.8433 - loss: 0.3672\n",
      "Epoch 12/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - accuracy: 0.8471 - loss: 0.3635\n",
      "Epoch 13/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - accuracy: 0.8434 - loss: 0.3703\n",
      "Epoch 14/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - accuracy: 0.8542 - loss: 0.3493\n",
      "Epoch 15/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - accuracy: 0.8523 - loss: 0.3548\n",
      "Epoch 16/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - accuracy: 0.8559 - loss: 0.3468\n",
      "Epoch 17/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 326us/step - accuracy: 0.8505 - loss: 0.3527\n",
      "Epoch 18/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 316us/step - accuracy: 0.8575 - loss: 0.3432\n",
      "Epoch 19/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - accuracy: 0.8532 - loss: 0.3609\n",
      "Epoch 20/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - accuracy: 0.8537 - loss: 0.3519\n",
      "Epoch 21/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - accuracy: 0.8503 - loss: 0.3588\n",
      "Epoch 22/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 290us/step - accuracy: 0.8575 - loss: 0.3392\n",
      "Epoch 23/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - accuracy: 0.8536 - loss: 0.3501\n",
      "Epoch 24/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317us/step - accuracy: 0.8602 - loss: 0.3419\n",
      "Epoch 25/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - accuracy: 0.8590 - loss: 0.3393\n",
      "Epoch 26/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - accuracy: 0.8588 - loss: 0.3426\n",
      "Epoch 27/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - accuracy: 0.8643 - loss: 0.3367\n",
      "Epoch 28/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - accuracy: 0.8571 - loss: 0.3510\n",
      "Epoch 29/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298us/step - accuracy: 0.8572 - loss: 0.3424\n",
      "Epoch 30/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - accuracy: 0.8555 - loss: 0.3446\n",
      "Epoch 31/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293us/step - accuracy: 0.8533 - loss: 0.3566\n",
      "Epoch 32/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - accuracy: 0.8575 - loss: 0.3471\n",
      "Epoch 33/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - accuracy: 0.8629 - loss: 0.3390\n",
      "Epoch 34/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - accuracy: 0.8635 - loss: 0.3367\n",
      "Epoch 35/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - accuracy: 0.8628 - loss: 0.3330\n",
      "Epoch 36/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - accuracy: 0.8612 - loss: 0.3303\n",
      "Epoch 37/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - accuracy: 0.8572 - loss: 0.3465\n",
      "Epoch 38/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315us/step - accuracy: 0.8645 - loss: 0.3378\n",
      "Epoch 39/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - accuracy: 0.8641 - loss: 0.3344\n",
      "Epoch 40/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - accuracy: 0.8662 - loss: 0.3337\n",
      "Epoch 41/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - accuracy: 0.8632 - loss: 0.3440\n",
      "Epoch 42/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466us/step - accuracy: 0.8643 - loss: 0.3320\n",
      "Epoch 43/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - accuracy: 0.8664 - loss: 0.3300\n",
      "Epoch 44/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - accuracy: 0.8636 - loss: 0.3312\n",
      "Epoch 45/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292us/step - accuracy: 0.8654 - loss: 0.3344\n",
      "Epoch 46/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - accuracy: 0.8639 - loss: 0.3407\n",
      "Epoch 47/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310us/step - accuracy: 0.8700 - loss: 0.3238\n",
      "Epoch 48/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 324us/step - accuracy: 0.8665 - loss: 0.3347\n",
      "Epoch 49/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - accuracy: 0.8648 - loss: 0.3307\n",
      "Epoch 50/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - accuracy: 0.8677 - loss: 0.3334\n",
      "Epoch 51/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - accuracy: 0.8616 - loss: 0.3378\n",
      "Epoch 52/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312us/step - accuracy: 0.8639 - loss: 0.3377\n",
      "Epoch 53/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310us/step - accuracy: 0.8602 - loss: 0.3347\n",
      "Epoch 54/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - accuracy: 0.8715 - loss: 0.3212\n",
      "Epoch 55/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313us/step - accuracy: 0.8689 - loss: 0.3235\n",
      "Epoch 56/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - accuracy: 0.8621 - loss: 0.3406\n",
      "Epoch 57/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - accuracy: 0.8662 - loss: 0.3360\n",
      "Epoch 58/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - accuracy: 0.8649 - loss: 0.3361\n",
      "Epoch 59/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - accuracy: 0.8688 - loss: 0.3298\n",
      "Epoch 60/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302us/step - accuracy: 0.8620 - loss: 0.3328\n",
      "Epoch 61/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - accuracy: 0.8649 - loss: 0.3332\n",
      "Epoch 62/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - accuracy: 0.8645 - loss: 0.3399\n",
      "Epoch 63/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313us/step - accuracy: 0.8687 - loss: 0.3303\n",
      "Epoch 64/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296us/step - accuracy: 0.8676 - loss: 0.3282\n",
      "Epoch 65/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - accuracy: 0.8649 - loss: 0.3315\n",
      "Epoch 66/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291us/step - accuracy: 0.8700 - loss: 0.3252\n",
      "Epoch 67/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306us/step - accuracy: 0.8733 - loss: 0.3212\n",
      "Epoch 68/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297us/step - accuracy: 0.8643 - loss: 0.3358\n",
      "Epoch 69/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - accuracy: 0.8678 - loss: 0.3329\n",
      "Epoch 70/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - accuracy: 0.8617 - loss: 0.3315\n",
      "Epoch 71/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - accuracy: 0.8673 - loss: 0.3287\n",
      "Epoch 72/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - accuracy: 0.8661 - loss: 0.3245\n",
      "Epoch 73/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - accuracy: 0.8633 - loss: 0.3412\n",
      "Epoch 74/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - accuracy: 0.8605 - loss: 0.3370\n",
      "Epoch 75/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - accuracy: 0.8666 - loss: 0.3305\n",
      "Epoch 76/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 301us/step - accuracy: 0.8669 - loss: 0.3281\n",
      "Epoch 77/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - accuracy: 0.8620 - loss: 0.3389\n",
      "Epoch 78/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295us/step - accuracy: 0.8625 - loss: 0.3323\n",
      "Epoch 79/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 304us/step - accuracy: 0.8624 - loss: 0.3359\n",
      "Epoch 80/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 303us/step - accuracy: 0.8657 - loss: 0.3234\n",
      "Epoch 81/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294us/step - accuracy: 0.8616 - loss: 0.3372\n",
      "Epoch 82/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311us/step - accuracy: 0.8647 - loss: 0.3408\n",
      "Epoch 83/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - accuracy: 0.8629 - loss: 0.3362\n",
      "Epoch 84/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - accuracy: 0.8691 - loss: 0.3192\n",
      "Epoch 85/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - accuracy: 0.8670 - loss: 0.3291\n",
      "Epoch 86/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 317us/step - accuracy: 0.8665 - loss: 0.3309\n",
      "Epoch 87/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - accuracy: 0.8642 - loss: 0.3338\n",
      "Epoch 88/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - accuracy: 0.8728 - loss: 0.3145\n",
      "Epoch 89/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 297us/step - accuracy: 0.8613 - loss: 0.3311\n",
      "Epoch 90/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 314us/step - accuracy: 0.8630 - loss: 0.3296\n",
      "Epoch 91/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 307us/step - accuracy: 0.8643 - loss: 0.3339\n",
      "Epoch 92/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - accuracy: 0.8584 - loss: 0.3427\n",
      "Epoch 93/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - accuracy: 0.8612 - loss: 0.3342\n",
      "Epoch 94/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 299us/step - accuracy: 0.8615 - loss: 0.3349\n",
      "Epoch 95/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305us/step - accuracy: 0.8606 - loss: 0.3279\n",
      "Epoch 96/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501us/step - accuracy: 0.8664 - loss: 0.3305\n",
      "Epoch 97/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 308us/step - accuracy: 0.8631 - loss: 0.3334\n",
      "Epoch 98/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300us/step - accuracy: 0.8588 - loss: 0.3392\n",
      "Epoch 99/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 313us/step - accuracy: 0.8609 - loss: 0.3331\n",
      "Epoch 100/100\n",
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 309us/step - accuracy: 0.8730 - loss: 0.3141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x3076737a0>"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To begin, we compile the network using the compile() method in TensorFlow.\n",
    "# For the optimizer, we’ll use the Adam optimizer, which is highly efficient for stochastic gradient descent. \n",
    "# This optimizer will adjust the weights during training to minimize the error between predicted and actual results. \n",
    "# For binary classification problems, the loss function to use is binary_crossentropy, \n",
    "# which calculates the error in predictions for binary outcomes. \n",
    "# If we were dealing with multiple categories, we’d use categorical cross-entropy instead. \n",
    "# Finally, we add accuracy as our evaluation metric to measure the performance of the model.\n",
    "ann.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Once compiled, the next step is to train the network using the fit() method. \n",
    "# We’ll specify the training data, batch size, and number of epochs. \n",
    "# Batch learning is more efficient because it processes small groups of data at once, \n",
    "# and the batch size of 32 is a commonly chosen value. \n",
    "# We’ll train the network for 100 epochs, which should give the model ample opportunity\n",
    "# to learn from the data without overfitting.\n",
    "ann.fit(X_train, y_train, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813f32e1-e6fd-4c03-bc5e-85248674c6bf",
   "metadata": {},
   "source": [
    "After training, we can observe the accuracy improving over the epochs, and it should converge at around 86% accuracy, meaning 86 out of 100 predictions are correct. \n",
    "\n",
    "Let's now use our ANN to make a prediction \n",
    "We want to predict if the customer with the following informations will leave the bank:\n",
    "- Geography: France\n",
    "- Credit Score: 600\n",
    "- Gender: Male\n",
    "- Age: 40 years old\n",
    "- Tenure: 3 years\n",
    "- Balance: 60000\n",
    "- Number of Products: 2\n",
    "- Does this customer have a credit card ? Yes\n",
    "- Is this customer an Active Member: Yes\n",
    "- Estimated Salary: 50000\n",
    "\n",
    "So, should we say goodbye to that customer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "1a25ace7-66c2-42a8-b034-21d57059ce47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
      "Predicted value: 0.026408588513731956\n",
      "The customer stays in the bank !\n"
     ]
    }
   ],
   "source": [
    "# Let's build our data for the prediction\n",
    "data_to_predict = [\n",
    "    1, 0, 0, # one hot encoded value for France\n",
    "    600, # credit score\n",
    "    1, # Label encoded value for Male\n",
    "    40, # Age\n",
    "    3, # Tenure\n",
    "    60000, # Balance\n",
    "    2, # Number of products\n",
    "    1, # Has credit card \n",
    "    1, # Is active member\n",
    "    50000 # Salary\n",
    "]\n",
    "\n",
    "# Once we’ve entered the customer's data and scaled it, we can run the predict method. \n",
    "# The result will be a probability—specifically, the probability that the customer will leave the bank. \n",
    "# To convert this probability into a final binary prediction (1 or 0), we can set a threshold, typically 0.5, \n",
    "# where a probability greater than 0.5 indicates the customer is likely to leave. \n",
    "# Running the code gives us a predicted probability of 0.02, meaning the customer is unlikely to leave the bank.\n",
    "prediction = ann.predict(sc.transform([data_to_predict]))\n",
    "prediction = prediction[0][0]\n",
    "print(f\"Predicted value: {prediction}\")\n",
    "print(\"The customer will leave the bank !\" if prediction > 0.5 else \"The customer stays in the bank !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981caae-fbeb-40d3-9b86-61b5a0f26324",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model’s overall performance on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "e60918c4-2cf7-43f4-974c-a5b2c671d58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 380us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.05083169],\n",
       "       [0.01757842],\n",
       "       [0.13310775],\n",
       "       ...,\n",
       "       [0.7128831 ],\n",
       "       [0.16193289],\n",
       "       [0.31699207]], dtype=float32)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We take all the test data and make a prediction for all of them\n",
    "y_pred = ann.predict(X_test)\n",
    "y_pred # y_pred contains probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "30147af6-e7a9-44e3-8144-658716bc2740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each prediction, we want to know if the user stays in the bank \n",
    "y_pred = y_pred > 0.5 # for every prediction, we turn the value into a binary value\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74feeaf8-1c45-4daf-8f12-652fe552d423",
   "metadata": {},
   "source": [
    "By comparing the predicted results to the actual outcomes, we generate a confusion matrix and compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "9073ae3b-b48f-4084-9190-171ddbc0112a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[1536   71]\n",
      " [ 203  190]]\n",
      "Accuracy score: 0.863\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"Confusion matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(f\"Accuracy score: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d77da6-f2fe-4f3f-95bd-f1d9ee2ddd11",
   "metadata": {},
   "source": [
    "In this case, the model achieved an accuracy of 86%, meaning 86 out of 100 predictions were correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8454cb7-9a9b-4f4d-b86c-8a43fd50e20e",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd7e564-0b75-4388-8fe9-f6f29b48578e",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2e658-6246-416f-85aa-a4ff6c4defa6",
   "metadata": {},
   "source": [
    "**Convolutional Neural Networks** (CNNs) are a powerful type of deep learning model designed to process and analyze visual data. Inspired by how the human brain processes images, CNNs automatically detect important features, such as edges, textures, or shapes, within an image. They are particularly effective for tasks like image classification, object detection, and facial recognition. CNNs use layers like convolution, pooling, and fully connected layers to progressively learn more complex patterns in images, making them essential for applications like self-driving cars, medical image analysis, and advanced computer vision systems.\n",
    "\n",
    "When you look at an image, your brain processes it based on the features you see. Depending on where your focus is, you may see different aspects. For instance, you might see a person looking at you or looking to the side, depending on which part of the image you focus on. This shows that our brains categorize objects based on the features they detect.\n",
    "\n",
    "![image](./dl/img/cnn_1.png)\n",
    "\n",
    "Another famous example is the image of a young woman or an old lady, where people see different figures depending on the features they notice first. These illusions demonstrate how our brain processes features and classifies objects. \n",
    "\n",
    "![image](./dl/img/cnn_2.png)\n",
    "\n",
    "Similarly, convolutional neural networks work by detecting and processing features in images, just like our brains do.\n",
    "CNNs are trained to classify images by learning from labeled data. For example, a trained CNN can correctly identify a cheetah in a picture with a high probability or classify a bullet train based on its features. However, sometimes the network might struggle with unclear images, just like we do, as it may not have enough information to make a confident prediction.\n",
    "\n",
    "For a black-and-white (grayscale) image, each pixel in the image is represented as a value between 0 and 255, where 0 is black, 255 is white, and values in between represent different shades of gray. This forms a 2D array (a matrix), where each pixel has a single value.\n",
    "\n",
    "![image](./dl/img/cnn_3.png)\n",
    "\n",
    "In the case of a colored image, each pixel has three values corresponding to the primary colors: Red, Green, and Blue (RGB). Each of these channels has an intensity value between 0 and 255, similar to the grayscale values. These three values together define the color of each pixel. So, a colored image is stored as a 3D array where each pixel has three layers (R, G, and B).\n",
    "\n",
    "![image](./dl/img/cnn_4.png)\n",
    "\n",
    "The 4 key steps in a Convolutional Neural Network (CNN) are:\n",
    "- **Convolution**: This step involves applying filters (or kernels) to the input image to detect specific features, such as edges, textures, or patterns. The filter slides across the image, performing a mathematical operation (convolution), and produces a feature map, highlighting important aspects of the image.\n",
    "- **Pooling (Max Pooling)**: Pooling reduces the size of the feature map by summarizing regions of the image. In max pooling, for example, the largest value from a group of pixels is taken, helping to retain the most important information while reducing computational complexity and overfitting.\n",
    "- **Flattening**: After the convolution and pooling layers, the 2D feature maps are transformed into a 1D vector. This flattening process prepares the data for the final step of classification by converting it into a format that fully connected layers can process.\n",
    "- **Full Connection (Fully Connected Layer)**: In this step, the flattened vector is passed through one or more fully connected layers. These layers perform the final classification by using the learned features from the previous steps to predict the output (e.g., identifying whether an image is of a cat or a dog).\n",
    "\n",
    "Together, these steps allow CNNs to automatically detect and classify visual patterns in images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71148174-b81e-4c0c-867a-d4bea3d7bc69",
   "metadata": {},
   "source": [
    "### Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e54740-8f68-4e92-ae37-7b1b1afdbc0d",
   "metadata": {},
   "source": [
    "In simple terms, convolution is a process where two functions combine to modify each other’s shape. It's widely used in fields like signal processing. The goal of convolution in CNNs is to detect important features in images and transform them into a more manageable form for further processing.\n",
    "\n",
    "The primary purpose of convolution is to **detect features** like edges, shapes, and patterns in the image. This helps preserve the spatial relationships between pixels, making it easier for the network to learn from the image while simplifying the data.\n",
    "\n",
    "CNNs use multiple filters to create multiple feature maps, each highlighting different features of the image. Even though some information is lost (since the image size is reduced), the essential parts—features important for classification—are preserved.\n",
    "\n",
    "**The Process of Convolution**:\n",
    "- **Input Image**: This is your starting image, which we often simplify to a matrix of 1s and 0s.\n",
    "- **Feature Detector (Filter/Kernel)**: A small matrix, often 3x3, that slides across the image. Different sizes like 5x5 or 7x7 are also used in various architectures. The filter extracts specific features from the image.\n",
    "- **Stride**: The step size for moving the filter across the image. A stride of 1 moves the filter one pixel at a time, while a stride of 2 reduces the image size further, helping make processing faster.\n",
    "- **Feature Map (Convolved Feature/Activation Map)**: The output of applying the filter to the image. This map highlights the presence of certain features while reducing the size of the image.\n",
    "\n",
    "The filter looks at small sections of the image and performs a simple math operation: it multiplies the numbers in the image by the numbers in the filter, and then it adds them up to get one new number. As the filter slides across the entire image, it creates a new, smaller image called a feature map. This feature map highlights certain patterns that the filter was looking for, like edges or corners.\n",
    "\n",
    "<img src=\"./dl/img/cnn_convolution.gif\" alt=\"image\" width=\"600px\">\n",
    "\n",
    "We'll end up with multiples Features Maps because we run this process with multiple differents filters. \n",
    "\n",
    "![image](./dl/img/cnn_5.png)\n",
    "\n",
    "The key idea is that convolution helps the network focus on the important parts of the image, like shapes or textures, while ignoring unnecessary details. It makes it easier for the neural network to understand what's in the image, whether it's detecting edges, patterns, or other important features.\n",
    "\n",
    "Here are some examples of images after convolution with differents filters:\n",
    "\n",
    "<img src=\"./dl/img/cnn_convolution_2.gif\" alt=\"image\" width=\"600px\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Key Takeaways**:\n",
    "- Convolution in CNNs is used to extract features from images.\n",
    "- The Feature Detector (or filter) scans the image, and its output forms the Feature Map.\n",
    "- The stride controls how much the image size is reduced.\n",
    "- Convolution preserves important patterns while reducing the complexity of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0e8ee5-d1c3-424c-82fa-d47d452815dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
