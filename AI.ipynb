{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c0e8817-db0b-4cc0-9028-f61a5ae63167",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad7d83b-1de1-4103-a18e-8eba24cea43d",
   "metadata": {},
   "source": [
    "The machine learning process generally follows a structured, step-by-step approach, which can be broken down into three main phases:\n",
    "\n",
    "1. Data Pre-processing:\n",
    "    - Import the data\n",
    "    - Clean the data\n",
    "    - Encoding data\n",
    "    - Split the data into training and test sets\n",
    "    - Feature scaling\n",
    "2. Modeling:\n",
    "    - Build the machine learning model\n",
    "    - Train the model on the training data\n",
    "    - Make predictions using the model\n",
    "3. Evaluation:\n",
    "    - Calculate performance metrics to assess how well the model performs\n",
    "    - Determine if the model is a good fit for the data and fulfills its intended purpose\n",
    "\n",
    "This process is key to building effective machine learning models, and throughout the examples, you'll gain practical experience applying these steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc681a-21fa-41d0-9ddf-b5bd66c14acb",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2d893-f29a-49c3-a137-aabbc144d341",
   "metadata": {},
   "source": [
    "We will explore in detail the **Pre-processing** process, which is common to all machine learning algorithms.\n",
    "It is important to master this pre-processing step before exploring the different algorithms.\n",
    "\n",
    "Data preprocessing is the crucial first step in any machine learning project because it transforms raw data into a clean, consistent format for the model:\n",
    "- Data Quality: Raw data often contains errors like missing values or duplicates. Preprocessing cleans this up to ensure the model learns from accurate data, leading to better predictions.\n",
    "- Consistency: Different formats across data sources can confuse the model. Preprocessing standardizes formats, ensuring consistent analysis.\n",
    "- Performance: Preprocessing highlights important features and reduces noise, enabling the model to find patterns more effectively and make more accurate predictions.\n",
    "- Efficiency: By simplifying data, preprocessing makes the model faster and more efficient, helping it focus on what matters most.\n",
    "\n",
    "Preprocessing ensures your data is clean, organized, and ready for the model, improving both accuracy and efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35563657-4d8c-4871-9653-46ec5648d3ad",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83869adb-63a7-4589-95f2-c0ebf4096fe0",
   "metadata": {},
   "source": [
    "The first step in building machine learning models is to import the necessary libraries. In this case, we’ll work with three essential libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e9a3324-2271-4026-bf52-2f11b7a8d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy helps us work with arrays, which are commonly used as input for machine learning models.\n",
    "# It is typically imported with the shortcut np.\n",
    "import numpy as np\n",
    "\n",
    "# matplotlib creates charts and graphs.\n",
    "# It is usually imported with the shortcut plt.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pandas is used for data manipulation and preprocessing, such as importing datasets\n",
    "# and creating matrices of features and dependent variables.\n",
    "# It is commonly imported with the shortcut pd.\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea60adb-6c74-4858-964f-f31313431e16",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ece0a9-2426-4ff2-a74a-51576c7b6186",
   "metadata": {},
   "source": [
    "To begin data pre-processing in machine learning, the first step is to import a dataset. In this example, we are working with a CSV file, data.csv, which contains customer information for a retail company, including country, age, salary, and whether they purchased a product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c4ccbb6-9e5f-4ffe-af8c-3e7210cf56d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Age</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>France</td>\n",
       "      <td>44.0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spain</td>\n",
       "      <td>27.0</td>\n",
       "      <td>48000.0</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Germany</td>\n",
       "      <td>30.0</td>\n",
       "      <td>54000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Spain</td>\n",
       "      <td>38.0</td>\n",
       "      <td>61000.0</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Germany</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Country   Age   Salary Purchased\n",
       "0   France  44.0  72000.0        No\n",
       "1    Spain  27.0  48000.0       Yes\n",
       "2  Germany  30.0  54000.0        No\n",
       "3    Spain  38.0  61000.0        No\n",
       "4  Germany  40.0      NaN       Yes"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the read_csv function from Pandas to import the dataset, which is typically a CSV file\n",
    "# This reads the file and stores it as a data frame, which holds all rows and columns of data from the CSV.\n",
    "dataset = pd.read_csv('./pp/Data.csv')\n",
    "dataset.head() # display the first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b17d4-b5c1-4fc6-b2e2-8303b41ac6b0",
   "metadata": {},
   "source": [
    "In any machine learning model, the dataset is split into two parts:\n",
    "- **Features** (Independent Variables): The information used to make predictions, typically the columns other than the last one.\n",
    "- **Dependent Variable**: The target outcome we want to predict, typically the last column in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78009775-61cf-4215-a771-6f673a0b67cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the features \"X\" include country, age, and salary\n",
    "X = dataset.iloc[:, :-1].values # get all the rows and get all the column except the last one\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dee0ddc2-1435-449a-8bf8-eebd5fcd6252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the dependent variable \"y\" is whether the customer purchased the product or not\n",
    "y = dataset.iloc[:, -1].values # get all the rows and get only the last column\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676a86c-786a-4d52-ad80-0df8de2fc47f",
   "metadata": {},
   "source": [
    "### Taking care of missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666a842-250d-4e5c-9b09-5517c692733c",
   "metadata": {},
   "source": [
    "Missing data can cause errors when training machine learning models, so it’s important to handle it properly. There are two common approaches:\n",
    "\n",
    "1. Ignoring or Deleting Missing Data:\n",
    "This method works well when the amount of missing data is minimal (e.g., less than 1%). Deleting a small percentage of data may not significantly impact the model's performance.\n",
    "\n",
    "2. Replacing Missing Data:\n",
    "A more common and reliable approach is to replace missing values with a calculated statistic, such as the mean of the column. This ensures that no data is lost, especially when there are large amounts of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d069d4fb-f085-4266-a348-dd59a1566c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As we can see here, we have missing age for Spain and missing salary for Germany\n",
    "# We want to replace this missing salary and age by the average\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f2e1b5d2-e068-46bd-be42-be95c4528ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the library that handle missing data\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an instance of SimpleImputer specifying the replacement strategy.\n",
    "# In this case we replace all missing values \"missing_values=np.nan\" with the mean \"strategy='mean'\"\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# The fit() method connects the imputer to the data by analyzing the columns with\n",
    "# numerical values and calculating statistics like the mean or median.\n",
    "# We give our last two columns that contains age and salary to the imputer\n",
    "imputer.fit(X[:, 1:3])\n",
    "\n",
    "# The transform() method applies the transformation to the data by replacing missing values\n",
    "# with the calculated statistics. The result is an updated matrix of features with no missing values.\n",
    "# Then we update the original matrix X, replacing the missing values in the specified columns with the mean values.\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "\n",
    "# By using both the fit() and transform() methods, you ensure that your matrix of features\n",
    "# is complete and ready for further processing, avoiding errors\n",
    "# in your machine learning model caused by missing data.\n",
    "# As we can see here, all the missing values were replaced by the mean of each column.\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fff3354-311e-408a-a9c0-0a387dfcac6a",
   "metadata": {},
   "source": [
    "### Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a3600f-235d-44c9-a0a7-e199258a41b6",
   "metadata": {},
   "source": [
    "In machine learning, we often need to convert categorical data (like country names) into numerical data so that models can process them. However, a simple numerical encoding (e.g., France = 0, Spain = 1, Germany = 2) can create unintended relationships between categories that don’t exist, as the model may incorrectly assume an order between these numbers.\n",
    "\n",
    "To avoid this issue, we use **one-hot encoding**, which transforms a single categorical column into multiple binary columns. For example, the \"country\" column with values like \"France,\" \"Spain,\" and \"Germany\" is converted into three binary columns:\n",
    "\n",
    "- France: **[1, 0, 0]**\n",
    "- Spain: **[0, 1, 0]**\n",
    "- Germany: **[0, 0, 1]**\n",
    "\n",
    "This method removes any implied numerical order between categories.\n",
    "\n",
    "Additionally, binary categorical data (such as \"yes\" or \"no\" values in the \"purchased\" column) can be safely encoded as 0 and 1 without negatively impacting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37c9196b-8f65-4a44-bcf5-3a22ea0f19f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0],\n",
       "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To perform one-hot encoding in Python, we use the ColumnTransformer and OneHotEncoder\n",
    "# These tools automate the conversion of categorical columns into binary columns,\n",
    "# allowing machine learning models to interpret and process them correctly.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Step 1: Let's now encode the features \"X\"\n",
    "\n",
    "\"\"\"\n",
    "We start by creating an object of the ColumnTransformer class.\n",
    "This object is responsible for applying transformations to specific columns.\n",
    "\n",
    "In the transformers argument, we specify three things:\n",
    "  - The type of transformation (e.g., \"encoder\" for one-hot encoding).\n",
    "  - The transformer to apply (e.g., OneHotEncoder()).\n",
    "  - The index of the column to transform (e.g., the \"country\" column, which has index 0).\n",
    "\n",
    "We also set the remainder argument to \"passthrough\" to ensure that columns not being transformed (e.g., age and salary) are retained.\n",
    "\"\"\"\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "\n",
    "\n",
    "# the fit_transform() method is used\n",
    "# to fit the ColumnTransformer to the data and apply the transformation in one step.\n",
    "# The result is an updated matrix of features with one-hot encoded columns.\n",
    "X = ct.fit_transform(X)\n",
    "\n",
    "# Since machine learning models expect the feature matrix (X) as a NumPy array,\n",
    "# we convert the result of fit_transform() to a NumPy array using np.array().\n",
    "X = np.array(X)\n",
    "\n",
    "# This process efficiently transforms categorical data into a suitable format\n",
    "# for machine learning models while ensuring that the rest of the feature matrix remains intact.\n",
    "# Let's check how our countries were transformed\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2bfb01d2-98f0-48f8-ac96-026b70073483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2: Let's now encode the dependant variable \"y\"\n",
    "\n",
    "\"\"\"\n",
    "for binary categorical data in the dependent variable (e.g., \"yes\" and \"no\"),\n",
    "we use label encoding.\n",
    "Label encoding converts these text values into binary numerical values:\n",
    "\n",
    "  - \"No\"  becomes 0\n",
    "  - \"Yes\" becomes 1\n",
    "\"\"\"\n",
    "\n",
    "# This transformation is done using the LabelEncoder class from scikit-learn.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# The fit_transform() method quickly converts the text into corresponding\n",
    "# numerical values for the dependent variable vector, which is now ready for model training.\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# By label encoding for binary outcomes, you ensure that categorical data is\n",
    "# in a format that machine learning models can process effectively.\n",
    "# Let's check the result of the transformation\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80771e9f-a757-44bb-82b8-2a0f26a14505",
   "metadata": {},
   "source": [
    "### Splitting the dataset into Training set and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4d8e7-2be5-4620-b903-80460feb3393",
   "metadata": {},
   "source": [
    "When preparing data for machine learning models, there are two key steps: **splitting the dataset** into training and test sets, and applying **feature scaling**.\n",
    "\n",
    "1. Splitting the Dataset, the dataset is split into two parts:\n",
    "    - Training Set: Used to train the model on existing observations.\n",
    "    - Test Set: Used to evaluate the performance of the model on new, unseen data, simulating future predictions.\n",
    "2. Feature Scaling:\n",
    "    - Feature scaling ensures that all features are on the same scale to prevent one feature from dominating others. This step adjusts the variables so that they take values within the same range, usually by normalizing or standardizing the data.\n",
    "\n",
    "**Feature Scaling must be applied AFTER splitting the dataset**\n",
    "\n",
    "The reason for this is to avoid **information leakage**. The test set is meant to represent new, unseen data. If you apply feature scaling before the split, you would be calculating the mean and standard deviation using the entire dataset, including the test set. This would allow the model to \"see\" the test data during training, which defeats the purpose of having a separate test set and leads to an inaccurate evaluation of model performance.\n",
    "\n",
    "By applying feature scaling after the split, you ensure that the test set remains untouched during training, preserving its role as new data to validate the model’s true performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9a985f4b-cc19-488b-a53d-3255e622363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To split the dataset, we use the train_test_split function from scikit-learn’s model_selection module.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\"\"\"\n",
    "The function train_test_split splits the dataset into four sets:\n",
    "  X_train: Matrix of features for the training set.\n",
    "  X_test:  Matrix of features for the test set.\n",
    "  y_train: Dependent variable for the training set.\n",
    "  y_test:  Dependent variable for the test set.\n",
    "\n",
    "The main parameters are:\n",
    "  X: Matrix of features.\n",
    "  y: Dependent variable vector.\n",
    "  test_size: Specifies the proportion of data to allocate to the test set (e.g., 20% or 0.2).\n",
    "  random_state: A fixed seed to ensure reproducibility of the split.\n",
    "\n",
    "The function returns four sets: X_train, X_test, y_train, and y_test.\n",
    "Typically, 80% of the data is assigned to the training set and 20% to the test set.\n",
    "This ratio ensures the model has enough data to learn from while still reserving a portion for evaluation.\n",
    "\"\"\"\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "dcb7070c-2af8-452b-a95e-31feaab3e9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, 35.0, 58000.0],\n",
       "       [1.0, 0.0, 0.0, 44.0, 72000.0],\n",
       "       [1.0, 0.0, 0.0, 48.0, 79000.0],\n",
       "       [0.0, 1.0, 0.0, 30.0, 54000.0],\n",
       "       [1.0, 0.0, 0.0, 37.0, 67000.0],\n",
       "       [0.0, 1.0, 0.0, 40.0, 63777.77777777778],\n",
       "       [0.0, 0.0, 1.0, 38.0, 61000.0],\n",
       "       [0.0, 0.0, 1.0, 38.77777777777778, 52000.0]], dtype=object)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3b2ca6b6-c5b5-41f7-a680-7864608062c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 50.0, 83000.0],\n",
       "       [0.0, 0.0, 1.0, 27.0, 48000.0]], dtype=object)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ee1255aa-c10d-469c-ba58-f1612d1febd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7e0c51f8-534b-4557-a7ed-e7c9a45c9bae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e804785-18fe-4413-b39b-cdd2b66bb09a",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490f1f0-6c2c-4e72-be03-6cc34c13808a",
   "metadata": {},
   "source": [
    "**Feature scaling** is the process of putting all features on the same scale to prevent certain features from dominating others due to differences in their magnitudes. However, feature scaling is not necessary for all machine learning models but is crucial for models sensitive to the scale of data, such as gradient-based algorithms (e.g., logistic regression, support vector machines).\n",
    "\n",
    "There are two Main Techniques:\n",
    "- Standardization:\n",
    "  - Formula: Subtract the mean of the feature and divide by the standard deviation.\n",
    "  - Result: Values are typically scaled between -3 and +3.\n",
    "  - Works well for all types of data, making it a reliable go-to method.\n",
    "- Normalization:\n",
    "  - Formula: Subtract the minimum value of the feature and divide by the range (maximum - minimum).\n",
    "  - Result: Values are scaled between 0 and 1.\n",
    "  - Best used when features follow a normal distribution.\n",
    "\n",
    "Feature scaling should always be applied **after splitting** the dataset into training and test sets. This is to prevent information leakage from the test set into the training process. Feature scaling is fitted on the training data (X_train), and then the same scaling transformation is applied to the test data (X_test), using the mean and standard deviation calculated from X_train.\n",
    "\n",
    "Practical Recommendation:\n",
    "    - Standardization is generally preferred because it works well across various scenarios and always improves model performance by ensuring consistent feature scaling.\n",
    "    - Normalization is ideal when you have normally distributed data.\n",
    "\n",
    "Standardization should only be applied to numerical features, not to dummy (one-hot encoded) variables.\n",
    "Dummy variables (e.g., binary 0s and 1s representing categorical data)\n",
    "are already on the same scale, and applying scaling to them would distort the interpretation of these values.\n",
    "\n",
    "Dummy variables already take values between 0 and 1, so scaling them is unnecessary\n",
    "and could result in the loss of interpretability.\n",
    "For instance, you may lose the ability to identify which binary values represent specific categories\n",
    "(e.g., countries). Moreover, applying scaling to dummy variables\n",
    "does not significantly improve model performance, and in most cases,\n",
    "it can even make the interpretation harder.\n",
    "\n",
    "Thus, feature scaling should be applied only to the numerical variables\n",
    "(e.g., age, salary) where the scale differs significantly.\n",
    "This ensures that your machine learning models are properly trained\n",
    "without distorting categorical information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "df0b032b-3b77-4f09-87d6-5924e1435d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To apply feature scaling, we use the StandardScaler class from the scikit-learn library,\n",
    "# which standardizes the features by subtracting the mean and dividing by the standard deviation.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initiate the scaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "\"\"\"\n",
    "X_train[:, 3:] means that we take all the rows and only the age and salary columns\n",
    "\n",
    "The first step is to fit the scaler on the training set (X_train) to compute\n",
    "the mean and standard deviation for each numerical feature.\n",
    "This process ensures that the scaling is based solely on the training data.\n",
    "You only need to scale the numerical columns.\n",
    "\n",
    "In this case, the numerical columns are the age and salary columns,\n",
    "which have indexes 3 and 4, respectively.\n",
    "We exclude the dummy (one hot encoded) variables from scaling.\n",
    "\"\"\"\n",
    "X_train[:, 3:] = sc.fit_transform(X_train[:, 3:])\n",
    "\n",
    "\"\"\"\n",
    "After applying feature scaling to the training set (X_train),\n",
    "the next step is to apply the same transformation to the test set (X_test).\n",
    "This is crucial to ensure that the test data is scaled using the same parameters\n",
    "(mean and standard deviation) computed from the training set.\n",
    "\n",
    "Since the test set simulates new, unseen data, you should only apply the transform()\n",
    "method (not fit_transform()), using the scaler that was fitted on the training set.\n",
    "This ensures consistency in scaling between the training and test sets,\n",
    "and prevents the model from learning any information from the test data (avoiding information leakage).\n",
    "\"\"\"\n",
    "X_test[:, 3:] = sc.transform(X_test[:, 3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195e5a6-6d65-40f2-b422-5e9840ee5347",
   "metadata": {},
   "source": [
    "After applying the scaling to both X_train and X_test, you can print them to verify that the numerical columns (e.g., age and salary) are now scaled between a common range, typically between -3 and +3 or -2 and +2, while dummy variables remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "75d38775-b078-43f6-8b8e-d40afd2702ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, -0.7529426005471072, -0.6260377781240918],\n",
       "       [1.0, 0.0, 0.0, 1.008453807952985, 1.0130429500553495],\n",
       "       [1.0, 0.0, 0.0, 1.7912966561752484, 1.8325833141450703],\n",
       "       [0.0, 1.0, 0.0, -1.7314961608249362, -1.0943465576039322],\n",
       "       [1.0, 0.0, 0.0, -0.3615211764359756, 0.42765697570554906],\n",
       "       [0.0, 1.0, 0.0, 0.22561095973072184, 0.05040823668012247],\n",
       "       [0.0, 0.0, 1.0, -0.16581046438040975, -0.27480619351421154],\n",
       "       [0.0, 0.0, 1.0, -0.013591021670525094, -1.3285009473438525]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d64f7bec-27ca-427f-8c7d-300434c47ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0, 1.0, 0.0, 2.1827180802863797, 2.3008920936249107],\n",
       "       [0.0, 0.0, 1.0, -2.3186282969916334, -1.7968097268236927]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d63bb-6962-4ff3-bbbb-ae25c1d11fe2",
   "metadata": {},
   "source": [
    "By properly scaling both the training and test sets, you ensure that your machine learning models are trained and evaluated under consistent conditions, leading to more accurate and reliable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa7f04-5abb-4433-b8ff-b19591eec576",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1fbaf-0ce0-4a17-9099-3a3f2668220c",
   "metadata": {},
   "source": [
    "The Internet is a massive global network that has transformed how we communicate and access information. In the early 90s, the concept of the Internet was new, and many people didn’t fully understand its potential. Back then, it was seen as something abstract and futuristic, but today it is a crucial part of our everyday lives. It connects individuals, organizations, and resources worldwide, allowing communication, sharing, and collaboration on an unprecedented scale.\n",
    "\n",
    "Neural networks and deep learning, much like the Internet, have existed for quite some time. The foundations of these technologies were laid in the 1960s and 70s, and by the 1980s, they were generating significant interest. Despite early enthusiasm, neural networks failed to deliver on their promise at that time, largely due to technological limitations. There wasn’t enough data, and computers were not powerful enough to process information at the scale required for neural networks to perform well. However, recent advancements in technology have led to a resurgence of interest in these fields, especially with the rise of deep learning, which is now transforming industries worldwide.\n",
    "\n",
    "The evolution of data storage has played a major role in this resurgence. In 1956, a 5 MB hard drive was so large that it had to be transported via forklift and way too expensive. Fast forward to nowadays, you could buy a 1 TB SSD that fits on your fingertip for a few bucks. This rapid expansion in storage capacity is part of the exponential growth that has defined technological progress in recent decades.\n",
    "\n",
    "Alongside data storage, the growth in computing power has followed Moore's Law, which observes that the processing power of computers doubles roughly every two years. In the 2020s, computers have achieved processing speeds comparable to the brain of a small mammal, like a rat. This explosion in computing capability is one of the key factors behind the rise of deep learning today, making it possible to train models on massive datasets.\n",
    "\n",
    "Deep learning itself is inspired by the structure of the human brain. It relies on artificial neural networks, which consist of several layers. The first is the input layer, where data is fed into the model. This data is processed through several hidden layers, which mimic the brain’s network of neurons. Information in the human brain doesn’t travel directly from input (senses) to output (actions or decisions). Instead, it passes through billions of interconnected neurons. Similarly, in deep learning, input data passes through hidden layers before reaching the output layer, where predictions or decisions are made.\n",
    "\n",
    "While early versions of neural networks only had one hidden layer, deep learning uses many hidden layers, allowing it to process complex data in ways that resemble how the brain works. This multi-layered architecture is what makes deep learning so powerful today, enabling machines to handle tasks like image recognition, speech processing, and language translation with unprecedented accuracy.\n",
    "\n",
    "![image](./dl/img/neural_net_intro.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce9bde-12ab-48e4-8cbe-9ebcf1c0bc0a",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf3751-9cfe-47a7-90fb-52a5a9843be0",
   "metadata": {},
   "source": [
    "### The neuron"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c66ef1d-ddc4-4166-a3ed-e41c52bdf5e3",
   "metadata": {},
   "source": [
    "The human brain is an incredible learning mechanism, and by replicating it in artificial systems, we hope to create similarly powerful learning tools for machines.\n",
    "\n",
    "Our first step in building neural networks is to recreate a neuron in a computer.\n",
    "In an artificial neural network, a neuron is connected to other neurons through synapses. Dendrites act as receivers, while axons transmit signals. Although axons and dendrites do not physically touch, they communicate across synapses, allowing the transfer of signals. In artificial networks, we simplify this by using input and output signals, representing synapses. We do not distinguish between axons and dendrites in the same way as we do in biology. Instead, we focus on the flow of information through the network, which is conceptually similar to how the brain operates.\n",
    "\n",
    "Neurons in machines receive input signals from other neurons, process these signals, and transmit an output signal. The input layer represents the values that are fed into the network, similar to how our senses provide information to the brain. In deep learning, the input values could be variables such as age, income, or any data relevant to the task at hand. These values are passed through the network, and each neuron's output contributes to the final result.\n",
    "\n",
    "It is important to standardize or normalize input values so that they fall within a similar range. This makes it easier for the neural network to process the data. There are different methods to achieve this, such as standardizing to a mean of zero and a variance of one, or normalizing data to fit within a range of zero to one. Both methods are essential for ensuring that the neural network functions optimally.\n",
    "\n",
    "The output of a neuron can be continuous (such as predicting a price), binary (such as predicting if a customer will leave a bank), or categorical (for multi-class classification tasks). In the case of categorical variables, the network generates multiple outputs representing each category.\n",
    "\n",
    "Each neuron is connected to other neurons through synapses, and these connections are assigned weights. The weights are essential because they determine the strength of the signal passed between neurons. The process of training a neural network involves adjusting these weights, allowing the network to learn which inputs are important and which are not. This is achieved through processes like backpropagation and gradient descent.\n",
    "\n",
    "Inside the neuron, the first step is to compute a weighted sum of all the input signals it receives. This sum is then processed through an activation function, which determines whether the neuron will pass the signal forward. This activation function plays a critical role in the network's ability to learn and make decisions. As signals pass through multiple neurons in the network, they undergo this process repeatedly, allowing the network to learn from data and make predictions.\n",
    "\n",
    "In summary, the neuron in an artificial neural network works by receiving input, applying weights, computing a sum, and passing the signal forward based on the activation function. This process is repeated across many neurons and synapses, enabling the network to learn from data and make accurate predictions.\n",
    "\n",
    "![image](./dl/img/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05623d9c-5057-449f-8cba-8279d917e30a",
   "metadata": {},
   "source": [
    "### The activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779ed164-eefb-484b-bc0d-87fdce898c3c",
   "metadata": {},
   "source": [
    "We're going to cover activation functions which play a crucial role in how signals are passed from one neuron to another in neural networks. \n",
    "\n",
    "There are several activation functions available, but we will focus on four key types commonly used in neural networks.\n",
    "\n",
    "The first is the **threshold function**, which is simple and rigid. If the weighted sum is less than zero, the output is zero; if it’s equal to or greater than zero, the output is one. This function acts as a binary switch but can be limiting due to its lack of flexibility.\n",
    "\n",
    "![image](./dl/img/treshold_function.png)\n",
    "\n",
    "Next is the **sigmoid function**, which follows a smooth curve. Its formula is 1/(1 + e^(-x)), where x represents the weighted sum. The sigmoid function is especially useful in output layers when predicting probabilities. Unlike the threshold function, the sigmoid provides a gradual transition between outputs, making it more flexible for binary classification problems.\n",
    "\n",
    "![image](./dl/img/sigmoid_function.png)\n",
    "\n",
    "The third is the **rectifier function**, or **ReLU** (Rectified Linear Unit), which has become one of the most popular activation functions in artificial neural networks. It outputs zero for negative values and gradually increases for positive input values. Despite its simple structure, ReLU is highly effective and widely used in hidden layers of neural networks due to its efficiency in handling large datasets.\n",
    "\n",
    "![image](./dl/img/rectifier_function.png)\n",
    "\n",
    "Finally, we have the **hyperbolic tangent (tanh) function**, which is similar to the sigmoid function but with outputs ranging from -1 to 1. This range can be useful in certain applications where negative values are required, offering more flexibility than the sigmoid function.\n",
    "\n",
    "![image](./dl/img/hyperbolic_function.png)\n",
    "\n",
    "These four activation functions serve different purposes depending on the network's architecture and the type of problem you're trying to solve. For binary classification tasks, the threshold and sigmoid functions are typically used. For example, the threshold function fits perfectly for binary variables, as it can only output zero or one. The sigmoid function, on the other hand, can also be applied in such cases but works by providing the probability of the output being one, similar to logistic regression.\n",
    "\n",
    "In more complex networks with multiple layers, it’s common to use a combination of activation functions. Typically, in hidden layers, the rectifier function is applied to process intermediate signals, while in the output layer, a sigmoid function is used to predict probabilities. This combination is effective and frequently used in neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca64bf6-be8a-4d0c-a5f0-e46e6aa3eda5",
   "metadata": {},
   "source": [
    "### Neural Networks Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3681cba-2fb4-4b0b-a8fb-bcad7dc4407f",
   "metadata": {},
   "source": [
    "With neural networks, you create a structure that allows the program to learn by itself. Instead of defining rules, you provide inputs and outputs, and the network figures out how to achieve the desired result on its own. A good analogy for this is teaching a network to distinguish between cats and dogs. In a rule-based approach, you'd define specific features like pointy ears for a cat or droopy ears for a dog. In contrast, a neural network learns by analyzing pre-labeled images and figures out the distinguishing features on its own.\n",
    "\n",
    "Let’s now look at a simple neural network called a perceptron. A perceptron was invented in 1957 by Frank Rosenblatt and is the most basic neural network structure. It consists of input values, weighted sums, and an output value, referred to as ŷ (y hat).\n",
    "\n",
    "![image](./dl/img/perceptron.png)\n",
    "\n",
    "To help the perceptron learn, we calculate the difference between the predicted output and the actual output using a cost function, which measures the error in the network’s predictions. A common cost function is half the squared difference between the predicted and actual values. The goal is to minimize this cost function, as a lower value means the network’s predictions are more accurate.\n",
    "\n",
    "![image](./dl/img/perceptron_1.png)\n",
    "\n",
    "After calculating the error, we use that information to update the weights in the network. Adjusting the weights allows the network to improve its predictions on the next iteration. Initially, we use one row of data to train the network, tweaking the weights continuously until the predicted value is close to the actual value, and the cost function is minimized.\n",
    "\n",
    "Next, we move to a full dataset, which consists of multiple rows of inputs and actual outputs. Each row represents a new observation (e.g., student exam data), and we feed these rows one by one into the same neural network. After processing all rows, we calculate the overall cost function, which sums the squared differences between the predicted and actual values across all rows. The network then updates its weights based on the total error. This process of feeding the entire dataset into the network and updating the weights is called one epoch. The network repeats this process for many epochs until the cost function is minimized and the network learns the optimal weights.\n",
    "\n",
    "This method, known as backpropagation, is fundamental to training neural networks. It enables the network to learn from its mistakes by adjusting the weights in response to errors, gradually improving its predictions. The goal is to find the minimum cost function, which signifies that the network has found the best possible weights for the given dataset.\n",
    "\n",
    "For additional reading on cost functions, a good resource is the article \"A List of Cost Functions Used in Neural Networks Alongside Applications\" on Cross Validated. It provides a comprehensive overview of different cost functions and their uses. With this knowledge of how neural networks learn, you’re now ready to move forward in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb3b32c-37fa-4f49-b298-58f03d0fad46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
